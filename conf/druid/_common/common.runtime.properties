# Extensions
druid.extensions.loadList=["druid-s3-extensions", "druid-histogram", "druid-datasketches",\
  "druid-lookups-cached-global", "mysql-metadata-storage", "druid-avro-extensions", "druid-parquet-extensions",\
  "druid-kafka-indexing-service", "statsd-emitter", "druid-knox-password-provider",\
  "druid-hash-name-numbered-shard-spec"]

# Logging
# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties=true

# Zookeeper
druid.zk.service.host=<DRUID_ZK_SERVICE_HOST>
druid.zk.paths.base=/<DRUID_CLUSTER_NAME>

# Metadata storage
druid.metadata.storage.type=mysql
druid.metadata.storage.connector.createTables=true
druid.metadata.storage.connector.connectURI=replicaset:/var/druid/metadata.uri:/<DRUID_CLUSTER_NAME>
druid.metadata.storage.connector.user=<MYSQL_USER>
druid.metadata.storage.connector.password={ "type": "knox", "knoxKey": "mysql:rbac:longqueryrw:credentials", "knoxUser": "kxlongqueryrw1804" }

# Deep storage
druid.storage.type=s3
druid.storage.bucket=pinterest-beijing
druid.storage.baseKey=<DRUID_CLUSTER_NAME>/segments

# Indexing service logs
# For local disk (only viable in a cluster if this is a network mount):
#druid.indexer.logs.type=file
#druid.indexer.logs.directory=var/druid/indexing-logs

# For S3:
druid.indexer.logs.type=s3
druid.indexer.logs.s3Bucket=pinterest-beijing
druid.indexer.logs.s3Prefix=<DRUID_CLUSTER_NAME>/indexing-logs

# Service discovery
druid.selectors.indexing.serviceName=<DRUID_CLUSTER_NAME>/overlord
druid.selectors.coordinator.serviceName=<DRUID_CLUSTER_NAME>/coordinator

# Caching
druid.cache.type=caffeine

# Querying
druid.server.http.maxIdleTime=PT5M
druid.server.http.defaultQueryTimeout=60000
druid.server.http.enableRequestLimit=false

# Monitoring
druid.monitoring.monitors=["org.apache.druid.java.util.metrics.JvmMonitor", "org.apache.druid.java.util.metrics.JvmCpuMonitor",\
  "org.apache.druid.java.util.metrics.JvmThreadsMonitor"]
druid.emitter=statsd
druid.emitter.statsd.hostname=localhost
druid.emitter.statsd.includeHost=true
druid.emitter.statsd.port=18127
druid.emitter.statsd.prefix=druid_metrics
druid.emitter.statsd.dimensionMapPath=conf/druid/_common/metricDimensions.json
druid.emitter.logging.logLevel=debug

# Storage type of double columns
# omitting this will lead to index double as float at the storage layer

druid.indexing.doubleStorage=double

# Drain processing queues in FIFO manner for requests with same priority
druid.processing.fifo=true
